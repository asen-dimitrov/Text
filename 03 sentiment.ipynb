{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##Класификация на текст\n",
    "\n",
    "Класификацията на текст е процес на присвояване на тагове или категории в текста според неговото съдържание. Това е една от основните задачи в обработката на естествени езици (NLP) с широки приложения като анализ на чувствата, определяне на теми, откриване на спам и откриване на намерения. Има много подходи за автоматична текстова класификация, които могат да бъдат групирани в три различни типа системи:\n",
    "\n",
    "- Системи, основани на правила\n",
    "- Системи, базирани на машинно обучение\n",
    "- Хибридни системи\n",
    "\n",
    "     \n",
    "Подходите, __основани на правила__, класифицират текста в организирани групи, като използват набор от предварително изготвени езикови правила. Тези правила инструктират системата да използва семантично релевантни елементи от текста, за да идентифицира съответните категории въз основа на неговото съдържание. Всяко правило се състои от модел и прогнозна категория.\n",
    "\n",
    "Системите, основани на правила, са разбираеми за хората и могат да бъдат подобрени с течение на времето. Но този подход има някои недостатъци. Като начало, тези системи изискват задълбочено познаване на темата на текста. Те също така отнемат много време, тъй като генерирането на правила е сложно може да бъде доста предизвикателно и обикновено изисква много анализи и тестове. Системите, основаващи се на правила са трудни за поддържане и не се оценяват добре, тъй като добавянето на нови правила може да повлияе на резултатите от съществуващите правила.\n",
    "\n",
    "Текстовата __класификация с машинно обучение__ се учи да прави класификации не въз основа на правила, а въз основа на минали наблюдения. Чрез използване на предварително маркирани примери като обучителни данни, алгоритъмът за машинно обучение може да научи различните асоциации между думите в текста и очакваната категория за конкретен входящ текст.\n",
    "\n",
    "Първата стъпка към обучението на класификатор с машинно обучение е извличането на характеристики: използва се метод числово представяне на текста под формата на вектор. Един от най-често използваните подходи е т. нар. торба с думи /bag of words/, където векторът представя честотата на дадена дума в предварително определен речник на думи.\n",
    "След като е обучен с достатъчно обучителни мостри, моделът за машинно обучение може да започне да прави точни прогнози. Моделът се използва за трансформиране на невиждан текст в набори от функции, които могат да се въвеждат в класификационния модел, за да се получат прогнози за темата на текста. \n",
    "Текстовата класификация с машинно обучение обикновено е много по-точна от подходите базирани на предварително зададени правила, особено при сложни задачи за класификация. Също така, класификаторите с машинно обучение са по-лесни за поддръжка и винаги можете да маркирате нови примери, за да научите нови задачи.\n",
    "\n",
    "__Хибридните системи__ комбинират основен класификатор, обучен с машинно обучение и система, основана на правила, която се използва за по-нататъшно подобряване на резултатите. Тези хибридни системи могат лесно да бъдат финно настроени чрез добавяне на специфични правила за проблемни теми, които не са правилно моделирани от базовия класификатор.\n",
    "\n",
    "Текстовата класификация може да се използва в широк диапазон от задачи, като класифициране на кратки текстове (заглавия или кратки съобщения) или организиране на много по-големи документи (например отзиви на клиенти, статии в медиите или юридически договори). Някои от най-известните примери за текстова класификация включват анализ на чувствата или настроенията __/sentiment analysis/__, етикетиране на теми __/topic labeling/__, разпознаване на език __/language detection/__ и откриване на намерения __/intent detection/__.\n",
    "\n",
    "Вероятно най-често срещаният пример за класификация на текста е __анализът на настроенията__: автоматизиран процес за определяне дали даден текст е положителен, отрицателен или неутрален. Компаниите използват класификатори на настроения за широк кръг от приложения, като например анализи на продукти, мониторинг на марката, поддръжка на клиенти, проучване на пазара, анализ на работната сила и много други.\n",
    "\n",
    "##Анализ на настроенията\n",
    "\n",
    "__Анализът на настроенията__, известен също като Opinion Mining, е поле в рамките на обработката на естествен език (NLP), който изгражда системи, които се опитват да идентифицират и извлекат мнения в текста. Обикновено, освен идентифициране на становището, тези системи извличат атрибути на израза например:\n",
    "\n",
    "- Полярност: ако говорителят изразява положително или отрицателно мнение.\n",
    "- Тема: предмета на обсъждане.\n",
    "- Притежателя на мнение: идентификация на лицето, което изразява становището.\n",
    "\n",
    "Анализът на настроенията е тема предмет на голям интерес и развитие, тъй като има много практически приложения. Тъй като публично и частно достъпна информация през интернет непрекъснато се увеличава, голям брой текстове, в които се изразяват мнения, са достъпни в сайтове за преглед, форуми, блогове и социални медии.\n",
    "\n",
    "С помощта на системите за анализ на настроенията тази неструктурирана информация може автоматично да се трансформира в структурирани данни за общественото мнение, за продукти, услуги, търговски марки, политика или всяка тема, за която хората могат да изразят мнение. Тези данни могат да бъдат много полезни за търговски приложения като маркетингов анализ, връзки с обществеността, обратна връзка за продуктите и обслужване на клиенти.\n",
    "\n",
    "##Видове анализ на настроенията\n",
    "\n",
    "Съществуват много видове анализ на настроенията и инструментите на варират от системи, които се фокусират върху полярността (положителна, отрицателна, неутрална) към системи, които откриват чувства и емоции (ядосани, щастливи, тъжни и т.н.) или идентифицират намеренията (напр. заинтересован/незаинтересован). \n",
    "\n",
    "Анализ на настроенията в дълбочина\n",
    "\n",
    "Понякога може да е необходимо по-точно определяне степента на полярност на мнението, така че вместо да говорите само за положителни, неутрални или отрицателни мнения, можете да се добавят и категории като силно положително или силно отрицателно.\n",
    "\n",
    "Възможно е и идентифицирането дали позитивните или отрицателните настроения са свързани с определено чувство, като например гняв, тъга или притеснения (т.е. отрицателни чувства) или щастие, любов или ентусиазъм (т.е. положителни чувства).\n",
    "\n",
    "Откриване на емоции\n",
    "\n",
    "Откриването на емоции има за цел да открие емоции като щастие, чувство на неудовлетвореност, гняв, тъга и други подобни. Много системи за откриване на емоции прибягват до лексикони (т.е. списъци с думи и емоциите, които изразяват) или сложни алгоритми за машинно обучение.\n",
    "\n",
    "Аспектно-базиран анализ на настроенията\n",
    "\n",
    "Обикновено, когато анализирате настроенията в предмети, например продукти, може да се интересувате не само от това дали хората говорят с положителен, неутрален или отрицателен тон за продукта, но и кои конкретни аспекти или характеристики на продукта имат предвид. \n",
    "\n",
    "##Процес на анализ на настроенията\n",
    "\n",
    "В __процеса на обучение__, моделът се научава да свързва определен вход (т.е. текст) със съответния изход (клас). Използват се извадка от предварително класифициран текст въз основа на която се обучава модел. От класифицирания текст се извеждат характеристики /feature extraction/ които служат за определяне на характерни черти на определения клас. На техническо ниво се създава числов вектор с думите срещани в обработвания документ, който модела може да обработи.\n",
    "\n",
    "__Класификатора на текст__ трансформира текстът представяйки го числово, обикновено с вектор. Всеки компонент на вектора представлява честотата на дадена дума или израз в предварително дефиниран речник (например целия корпус на изследваните документи). Този процес е известен като извличане на характеристика или векторизиране на текст, най-често използвания подход е наречен bag of words /чанта с думи/.\n",
    "\n",
    "В __процеса на прогнозиране__ екстракторът на характеристики се използва за трансформиране на нов текст във вектори на срещаните думи. Тези вектори се подават в модела /класификационен алгоритъм/, който генерира прогнозни класове (отново положителни, отрицателни или неутрални). Някой от __класификационените алгоритми__, които могат да се използват са:\n",
    "\n",
    "- Наивeн Бeйс / Naïve Bayes/: семейство вероятностни алгоритми, които използват теоремата на Бейс, за да предскажат категорията на текста.\n",
    "\n",
    "- Линейна регресия: много добре познат алгоритъм в статистиката, използван за прогнозиране на стойности от характеристики.\n",
    "\n",
    "- Метод на опорните точки /Support Vector Machines/: не-вероятностен модел, който използва представяне на текстови примери като точки в многоизмерно пространство. Тези примери са картографирани така, че примерите на различните категории (чувства) принадлежат към отделни региони на това пространство. След това, новите текстове се картографират в същото пространство и се предсказват, че принадлежат към категория, въз основа на коя област попадат.\n",
    "\n",
    "- Дълбоко обучение /Deep Learning/: разнообразен набор от алгоритми, които се опитват да имитират как работи човешкият мозък, използвайки изкуствени невронни мрежи за обработка на данни.    \n",
    "\n",
    "Основни понятия при анализа на текст:\n",
    "\n",
    "__regular expressions__ /регулярен израз/ - последователност от знаци, която дефинира шаблон за търсене. Обикновено този шаблон се използва от алгоритми за претърсване на низове за операции от типа „търсене“ или „търсене и заместване“ върху низове или за проверка на валидността на въведени данни.\n",
    "\n",
    "__segmentation/tokenization__ /разделяне на текст/ - процес на разделяне на писмен текст на значими единици, като думи, изречения или теми.\n",
    "\n",
    "__case folding__ - процесът на конвертиране на всички символи в документ в един и същи размер, или в главни или малки букви.\n",
    "\n",
    "__lemmatization__ - процесът на групиране на различните форми на дадена дума, така че те да могат да бъдат анализирани като отделен елемент, идентифициран от корена на думата.\n",
    "\n",
    "__stemmization__ - процесът на премахвана на приставки и наставки от думата. Крайния резултат може да не е дума със самостоятелно значение но все пак е полезна при анализа.\n",
    "\n",
    "__Porter's algorithm__ - алгоритъмът на Портър е най-често използваният алгоритъм за премахвана на приставки и наставки от думата в английския език.\n",
    "__corpus__ - голям и структуриран набор от текстове използва се за извършване на статистически анализи и тестване на хипотези, проверка на събития или утвърждаване на езикови правила.\n",
    "__stop words__ - думи, които се филтрират преди или след обработка на текст. Обикновено се отнасят до най-често срещаните думи на даден език, които не носят значение.\n",
    "\n",
    "\n",
    "#Предмет на курсовата работа\n",
    "\n",
    "Предмет на настоящата курсова работа е анализ на настроенията с инструменти за анализ на текст. Като работна среда е използван езика Python. Основната библиотека използвана за обработка на текста е NLTK /Natural Language Toolkit/. Използвана е предварителна обработка на текста: сегментиране на отделните думи в текста, премахване на стоп думи, премахване на приставки и наставки с алгоритъма на Портър. Извличане на характеристики, чрез bag of words. Като класификационен алгритъм е използван Наивен Бейс. \n",
    "\n",
    "Данните предмет на анализа са извадка от филмови ревюта от сайта www.rottentomatoes.com. Данните са на английски. Съдържат извадка от 10 000 ревюта на филми. Половината от тях са положителни, а другата половина отрицателни. Данните са организирани в две колони \"Freshness\" съдържа оценката, а \"Review\" самото ревю.  За обозначаване на положителните ревюта е използвана категорията \"fresh\", а за отрицателните категорията \"rotten\". Няма липсващи стойности.\n",
    "\n",
    "##Изпълнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Започвам с импортиране на някой от необходимите библиотеки. Pandas за работа с данни в табличен вид. RE за работа с регулярни изрази.И части от NLTK  за специфични задачи word_tokenize, PorterStemmer, stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Импортирам данните в data frame. Извеждам измеренията и показвам първите няколко реда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = pd.read_csv('rotten_tomatoes_reviews_sample.csv')\n",
    "md.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freshness</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rotten</td>\n",
       "      <td>The muddled mental states of the characters (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fresh</td>\n",
       "      <td>The Big Wedding is an occasionally charming a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fresh</td>\n",
       "      <td>While 'Captain Phillips' is certainly a thril...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rotten</td>\n",
       "      <td>A more mean-spirited and tongue-in-cheek B-mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fresh</td>\n",
       "      <td>It's easy to poke fun at some of the more far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rotten</td>\n",
       "      <td>This is a superficial film, a condensation of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rotten</td>\n",
       "      <td>About the only good call in The Girl on the T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fresh</td>\n",
       "      <td>The Drop is the last film Gandolfini made bef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fresh</td>\n",
       "      <td>It's sweet and superficial, but the meanderin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fresh</td>\n",
       "      <td>Technically, artistically and emotionally, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Freshness                                             Review\n",
       "0    rotten   The muddled mental states of the characters (...\n",
       "1     fresh   The Big Wedding is an occasionally charming a...\n",
       "2     fresh   While 'Captain Phillips' is certainly a thril...\n",
       "3    rotten   A more mean-spirited and tongue-in-cheek B-mo...\n",
       "4     fresh   It's easy to poke fun at some of the more far...\n",
       "5    rotten   This is a superficial film, a condensation of...\n",
       "6    rotten   About the only good call in The Girl on the T...\n",
       "7     fresh   The Drop is the last film Gandolfini made bef...\n",
       "8     fresh   It's sweet and superficial, but the meanderin...\n",
       "9     fresh   Technically, artistically and emotionally, th..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В следващата клетка създавам функция за предварителна обработка на текста. Променливата stop_words съдържа всички стоп думи в английския език от библиотеката nltk.corpus, към тях добавям и пунктуационните знаци.\n",
    "Функцията tokenize приема входящи данни и извършва поредица от действия обозначени с коментара в текста:\n",
    "1. Премахва номера, пункуация и долна черта, замествайки ги с интервал. Командата е от библиотеката re.\n",
    "2. Разделя текста на думи с nltk.tokenize.\n",
    "3. Премахва стоп думите, чрез итерация.\n",
    "4. Премахва приставките и наставките с алгоритъма на Портър от nltk.stem.porter, чрез итерация.\n",
    "5. Обединява получения от предната стъпка списък /list/ в низ /sring/. Необходимо е защото следващата стъпка го изисква."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) \n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(text):\n",
    "    text = re.sub(r'\\W+|\\d+|_', ' ', text)                       # 1\n",
    "    tokens = nltk.word_tokenize(text)                            # 2\n",
    "    tokens = [word for word in tokens if not word in stop_words] # 3\n",
    "    tokens = [stemmer.stem(word) for word in tokens]             # 4\n",
    "    tokens = \" \".join(tokens)                                    # 5   \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавям колона 't_Review', в която прилагам функцията tokenize върху всеки елемент от колоната 'Review'. Показвам отново размерана данните. Показвам сравнение между първото ревю преди и след прилагане на функцията."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n",
      "\n",
      " before tokenize:\n",
      "  The muddled mental states of the characters (and that of the audience) are mirrored by the film's visual style, which oscillates between languorous dreamscapes and frenetic action sequences.\n",
      "\n",
      " after tokenize:\n",
      " the muddl mental state charact audienc mirror film visual style oscil languor dreamscap frenet action sequenc\n"
     ]
    }
   ],
   "source": [
    "md['t_Review'] = md['Review'].apply(tokenize)\n",
    "print(md.shape)\n",
    "print('\\n before tokenize:\\n',md.Review[0])\n",
    "print('\\n after tokenize:\\n',md.t_Review[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Използвам CountVectorizer от sklearn.feature_extraction.text за да създам матрица която да предтави наличието на думите във всеко ревю. Резултата е матрица с размери 14 545 на 10 000, в която повечето стойности са 0. Запазвам я като датафрейм в променливата mv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 14545)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aardman</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aarp</th>\n",
       "      <th>ab</th>\n",
       "      <th>abair</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abattoir</th>\n",
       "      <th>abbass</th>\n",
       "      <th>abbey</th>\n",
       "      <th>...</th>\n",
       "      <th>zwigoff</th>\n",
       "      <th>zzzzzzzzzzzzzz</th>\n",
       "      <th>élan</th>\n",
       "      <th>époqu</th>\n",
       "      <th>était</th>\n",
       "      <th>étude</th>\n",
       "      <th>évocateur</th>\n",
       "      <th>être</th>\n",
       "      <th>ótima</th>\n",
       "      <th>über</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14545 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaliyah  aardman  aaron  aarp  ab  abair  abandon  abattoir  abbass  abbey  \\\n",
       "0        0        0      0     0   0      0        0         0       0      0   \n",
       "1        0        0      0     0   0      0        0         0       0      0   \n",
       "2        0        0      0     0   0      0        0         0       0      0   \n",
       "3        0        0      0     0   0      0        0         0       0      0   \n",
       "4        0        0      0     0   0      0        0         0       0      0   \n",
       "\n",
       "   ...  zwigoff  zzzzzzzzzzzzzz  élan  époqu  était  étude  évocateur  être  \\\n",
       "0  ...        0               0     0      0      0      0          0     0   \n",
       "1  ...        0               0     0      0      0      0          0     0   \n",
       "2  ...        0               0     0      0      0      0          0     0   \n",
       "3  ...        0               0     0      0      0      0          0     0   \n",
       "4  ...        0               0     0      0      0      0          0     0   \n",
       "\n",
       "   ótima  über  \n",
       "0      0     0  \n",
       "1      0     0  \n",
       "2      0     0  \n",
       "3      0     0  \n",
       "4      0     0  \n",
       "\n",
       "[5 rows x 14545 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec1 = CountVectorizer()\n",
    "mv = pd.DataFrame(countvec1.fit_transform(md['t_Review']).toarray(), columns=countvec1.get_feature_names(), index=None)\n",
    "mv['class'] = md['Freshness']\n",
    "print(mv.shape)\n",
    "mv.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чрез train_test_split от sklearn.model_selection разделям датафрейм mv на две. Множество за обучение и за тест като данните за тестване са 20% от всички. Задавам random_state = 0 за да се възпроизвеждат еднакви резултати при всяко стартиране."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 14545)\n",
      "(2000, 14545)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "mv_train, mv_test = train_test_split(mv, test_size=0.2, random_state = 0)\n",
    "print(mv_train.shape)\n",
    "print(mv_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Създавам инстанция на MultinomialNB от sklearn.naive_bayes и прилагам модела върху данните."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "X_train= mv_train.drop(['class'], axis=1)\n",
    "\n",
    "clf.fit(X_train, mv_train['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Показаната стойност е средната точност при прогнозиране на данните."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7325"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test= mv_test.drop(['class'], axis=1)\n",
    "clf.score(X_test,mv_test['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извеждам и матрицата с грешни и правилно класифицирани наблюдение. По основния диагонал са правилно прогнозираните категории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[891 115]\n",
      " [126 868]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(mv_test['class'], clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "И накрая извършвам реални прогнози въз основа на тестовото множество. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rotten' 'fresh' 'fresh' ... 'fresh' 'fresh' 'rotten']\n"
     ]
    }
   ],
   "source": [
    "pred_sentiment=clf.predict(mv_test.drop('class', axis=1))\n",
    "print(pred_sentiment)"
   ]
  },
  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Източници\n",
    "\n",
    "##Литература:\n",
    "1. https://www.ibm.com/support/knowledgecenter/en/SS3RA7_18.2.1/ta_guide_ddita/textmining/shared_entities/tm_intro_tm_defined.html\n",
    "2. https://monkeylearn.com/\n",
    "3. https://bg.wikipedia.org/wiki/%D0%A0%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B5%D0%BD_%D0%B8%D0%B7%D1%80%D0%B0%D0%B7\n",
    "4. https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation\n",
    "5. https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "6. https://www.nltk.org/book/\n",
    "7. Python Text Processing with NLTK 2.0 Cookbook, Jacob Perkins, Packt Publishing Ltd.\n",
    "\n",
    "##Данни:\n",
    "1. https://github.com/nicolas-gervais/6-607-Algorithms-for-Big-Data-Analysis/blob/master/rotten_tomatoes_reviews_sample.csv\n",
    "2. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\n",
    "3. https://www.rottentomatoes.com/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
